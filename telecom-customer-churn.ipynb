{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"source":"<a href=\"https://www.kaggle.com/code/mohamedchahed/telecom-customer-churn?scriptVersionId=124399611\" target=\"_blank\"><img align=\"left\" alt=\"Kaggle\" title=\"Open in Kaggle\" src=\"https://kaggle.com/static/images/open-in-kaggle.svg\"></a>","metadata":{},"cell_type":"markdown"},{"cell_type":"markdown","source":"# ðŸ“– Background","metadata":{"id":"PiTiNiNXXNbY"}},{"cell_type":"markdown","source":"The dataset being used in this study is sourced from a telecom company based in Iran. The dataset includes information on customer behavior over a one-year period, with each row representing a different customer. In addition to a churn label, the dataset also includes data on various activities undertaken by the customers, such as call failures and subscription length. The aim of the study is to analyze this dataset and explore patterns or trends that may help the company improve customer retention rates. This type of analysis is important for telecom companies to ensure that they are providing high-quality services and meeting the needs of their customers. By leveraging the insights gained from this analysis, the company can make informed decisions and take proactive measures to reduce customer churn and improve overall customer satisfaction.","metadata":{"id":"qGQb6_999q8t"}},{"cell_type":"markdown","source":"# ðŸ’¾ Data","metadata":{"id":"Tlr7pR3095A6"}},{"cell_type":"code","source":"import pandas as pd\ndf = pd.read_csv('../input/customer-churn/customer_churn.csv')\ndf.head()","metadata":{"id":"J22KQdKiBvsy","outputId":"6b0ebfa8-f0f9-4b73-b51d-c8e6b28225cb","execution":{"iopub.status.busy":"2023-03-30T16:57:05.94183Z","iopub.execute_input":"2023-03-30T16:57:05.942286Z","iopub.status.idle":"2023-03-30T16:57:05.99613Z","shell.execute_reply.started":"2023-03-30T16:57:05.942245Z","shell.execute_reply":"2023-03-30T16:57:05.994795Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"This dataset contains information for 3150 customers of an Iranian telecom company, collected randomly over 12 months. The dataset consists of 13 columns, including call failures, frequency of SMS, number of complaints, subscription length, age group, charge amount, type of service, seconds of use, status, frequency of use, and customer value. All attributes except churn represent data from the first 9 months, with churn labels representing customer states at the end of the 12 months, leaving a 3-month planning gap. The dataset aims to help the company improve customer retention rates and satisfaction through analysis of customer behavior.\n\n\n\n---\n\n\n| Column                  | Explanation                                             |\n|-------------------------|---------------------------------------------------------|\n| Call Failure            | number of call failures                                 |\n| Complaints              | binary (0: No complaint, 1: complaint)                  |\n| Subscription Length     | total months of subscription                            |\n| Charge Amount           | ordinal attribute (0: lowest amount, 9: highest amount) |\n| Seconds of Use          | total seconds of calls                                  |\n| Frequency of use        | total number of calls                                   |\n| Frequency of SMS        | total number of text messages                           |\n| Distinct Called Numbers | total number of distinct phone calls                    |\n| Age Group               | ordinal attribute (1: younger age, 5: older age)        |\n| Tariff Plan             | binary (1: Pay as you go, 2: contractual)               |\n| Status                  | binary (1: active, 2: non-active)                       |\n| Age                     | age of customer                                         |\n| Customer Value          | the calculated value of customer                        |\n| Churn                   | class label (1: churn, 0: non-churn)                    |\n\n\n---\n\n\n\nSource : https://archive.ics.uci.edu/ml/datasets/Iranian+Churn+Dataset","metadata":{"id":"5iZBwr1U-Wvy"}},{"cell_type":"markdown","source":"# ðŸ“Š EDA\n","metadata":{"id":"AKuQnsMF_zMd"}},{"cell_type":"markdown","source":"### Anomalies","metadata":{"id":"q_xTrw1YyQOu"}},{"cell_type":"markdown","source":"#### Missing values","metadata":{"id":"IHXCLjAHyfWR"}},{"cell_type":"code","source":"# check for missing values\ndf.isnull().sum()","metadata":{"id":"ooOtJw4byjBc","outputId":"727b9fa7-75d3-42ee-d6ac-99cd1bf64aea","execution":{"iopub.status.busy":"2023-03-30T16:58:47.867877Z","iopub.execute_input":"2023-03-30T16:58:47.868301Z","iopub.status.idle":"2023-03-30T16:58:47.878221Z","shell.execute_reply.started":"2023-03-30T16:58:47.868266Z","shell.execute_reply":"2023-03-30T16:58:47.877371Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"#### Outliers Detection with Z-score","metadata":{"id":"ivwg-gtU25V3"}},{"cell_type":"code","source":"import numpy as np\n# Calculate the mean and standard deviation for each variable\nmean_call_failure = df['Call Failure'].mean()\nstd_call_failure = df['Call Failure'].std()\n\nmean_subscription_length = df['Subscription Length'].mean()\nstd_subscription_length = df['Subscription Length'].std()\n\nmean_seconds_of_use = df['Seconds of Use'].mean()\nstd_seconds_of_use = df['Seconds of Use'].std()\n\nmean_frequency_of_use = df['Frequency of use'].mean()\nstd_frequency_of_use = df['Frequency of use'].std()\n\nmean_frequency_of_SMS = df['Frequency of SMS'].mean()\nstd_frequency_of_SMS = df['Frequency of SMS'].std()\n\nmean_distinct_called_numbers = df['Distinct Called Numbers'].mean()\nstd_distinct_called_numbers = df['Distinct Called Numbers'].std()\n\nmean_customer_value = df['Customer Value'].mean()\nstd_customer_value = df['Customer Value'].std()\n\n# Calculate the Z-scores for all data points\nz_scores_call_failure = np.abs((df['Call Failure'] - mean_call_failure) / std_call_failure)\nz_scores_subscription_length = np.abs((df['Subscription Length'] - mean_subscription_length) / std_subscription_length)\nz_scores_seconds_of_use = np.abs((df['Seconds of Use'] - mean_seconds_of_use) / std_seconds_of_use)\nz_scores_frequency_of_use = np.abs((df['Frequency of use'] - mean_frequency_of_use) / std_frequency_of_use)\nz_scores_frequency_of_SMS = np.abs((df['Frequency of SMS'] - mean_frequency_of_SMS) / std_frequency_of_SMS)\nz_scores_distinct_called_numbers = np.abs((df['Distinct Called Numbers'] - mean_distinct_called_numbers) / std_distinct_called_numbers)\nz_scores_customer_value = np.abs((df['Customer Value'] - mean_customer_value) / std_customer_value)\n\n# Set threshold for detecting outliers\noutliers_call_failure = df[z_scores_call_failure > 3.5]\noutliers_subscription_length = df[z_scores_subscription_length > 3.5]\noutliers_seconds_of_use = df[z_scores_seconds_of_use > 3.5]\noutliers_frequency_of_use = df[z_scores_frequency_of_use > 3.5]\noutliers_frequency_of_SMS = df[z_scores_frequency_of_SMS > 3.5]\noutliers_distinct_called_numbers = df[z_scores_distinct_called_numbers > 3.5]\noutliers_customer_value = df[z_scores_customer_value > 3.5]\n\n# Print the results\nprint(outliers_call_failure['Call Failure'])\nprint(outliers_subscription_length['Subscription Length'])\nprint(outliers_seconds_of_use['Seconds of Use'])\nprint(outliers_frequency_of_use['Frequency of use'])\nprint(outliers_frequency_of_SMS['Frequency of SMS'])\nprint(outliers_distinct_called_numbers['Distinct Called Numbers'])\nprint(outliers_customer_value['Customer Value'])","metadata":{"id":"rym1SN5V3AA_","outputId":"cb8a1465-28c6-4c11-aa97-664fecee55d3","execution":{"iopub.status.busy":"2023-03-30T16:58:50.975279Z","iopub.execute_input":"2023-03-30T16:58:50.975665Z","iopub.status.idle":"2023-03-30T16:58:51.013652Z","shell.execute_reply.started":"2023-03-30T16:58:50.975631Z","shell.execute_reply":"2023-03-30T16:58:51.012485Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Based on the Z-score method, we have detected some outliers in the numerical columns of our dataset. However, after reviewing these outliers, there is no evidence to suggest that these values are incorrect or outliers in the traditional sense. Therefore, we can conclude that the outliers detected by the Z-score method are valid data points and should not be removed or modified.","metadata":{"id":"gH_OfGAg5d1h"}},{"cell_type":"markdown","source":"### Univariate Analysis","metadata":{"id":"YNQkHjS1xrFE"}},{"cell_type":"markdown","source":"#### Distributions ","metadata":{"id":"NJ0nUryBDNpS"}},{"cell_type":"code","source":"import seaborn as sns\nimport matplotlib.pyplot as plt\n\n# Select the columns to plot\ncols = ['Subscription Length', 'Seconds of Use',\n        'Frequency of use', 'Frequency of SMS', 'Distinct Called Numbers',\n        'Customer Value']\n\n# Set the figure size and layout\nfig, axes = plt.subplots(nrows=2, ncols=3, figsize=(20,10))\naxes = axes.flatten()\n\n# Plot the distribution of each column\nfor i, col in enumerate(cols):\n    sns.histplot(data=df, x=col, kde=True, ax=axes[i])\n    \n# Add titles and adjust spacing\nplt.suptitle('Distribution of Features')\nplt.tight_layout()","metadata":{"id":"Uy2BX_8r9bw1","outputId":"f6faba4d-b033-4b22-fdd2-02c8cfdfc776","execution":{"iopub.status.busy":"2023-03-30T16:58:58.083503Z","iopub.execute_input":"2023-03-30T16:58:58.083947Z","iopub.status.idle":"2023-03-30T16:59:00.684345Z","shell.execute_reply.started":"2023-03-30T16:58:58.083906Z","shell.execute_reply":"2023-03-30T16:59:00.682928Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"After plotting the distributions and observing that they are skewed, it's clear that preprocessing is needed to make the data more suitable for modeling. Skewed data can cause issues with statistical methods and make it difficult to identify meaningful patterns in the data. Therefore, it's important to transform the data to reduce the impact of outliers and bring the distribution closer to a normal distribution.\n\n","metadata":{"id":"mYyNr5ufARCA"}},{"cell_type":"markdown","source":"#### Target Feature Label Imbalance","metadata":{"id":"7l215Lo-Dsni"}},{"cell_type":"code","source":"# Barplot\nax = sns.barplot(x='Churn', y='Churn', data=df, estimator=lambda x: len(x) / len(df) * 100)\n\n# Adding labels and title\nplt.xlabel('Churn')\nplt.ylabel('Percentage')\nplt.title('Distribution of Churn')\n\n# Adding percentage values to the bars\nfor p in ax.patches:\n    ax.text(p.get_x() + p.get_width() / 2, \n            p.get_height(), \n            '{:.2f}%'.format(p.get_height()), \n            ha='center', \n            va='bottom')\n\nplt.show()","metadata":{"id":"6ePsdleS9b8L","outputId":"fe66941b-ea1a-4833-899a-54e6e4d80de8","execution":{"iopub.status.busy":"2023-03-30T16:59:45.34333Z","iopub.execute_input":"2023-03-30T16:59:45.343858Z","iopub.status.idle":"2023-03-30T16:59:45.617723Z","shell.execute_reply.started":"2023-03-30T16:59:45.343797Z","shell.execute_reply":"2023-03-30T16:59:45.6164Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"It is clear that the target feature 'churn' is imbalanced, with a higher proportion of customers not churning compared to those who do. This imbalance can pose a problem during the modeling stage, as most machine learning algorithms are designed to maximize accuracy and may be biased towards the majority class. To address this issue, we need to use resampling techniques such as oversampling or undersampling to balance the classes in the training data.","metadata":{"id":"XNRq9nRFEzBC"}},{"cell_type":"markdown","source":"### Multivariate Analysis","metadata":{"id":"JYjv8S2JxwCM"}},{"cell_type":"code","source":"import seaborn as sns\nimport matplotlib.pyplot as plt\n# Correlations analysis\n#plot\ncmap = sns.color_palette('YlOrBr', as_cmap=True)\n\nplt.figure(figsize = (18, 8))\nsns.heatmap(data = df.corr(), annot = True, cmap = cmap)\n\n#customize\nplt.title('Correlation Heatmap',fontsize = 20, fontweight = 'bold')\n\nplt.show() ","metadata":{"id":"gUn_prIxxTD4","outputId":"b1af89d1-5f9d-4e85-8424-29ccb20cc3c6"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"we can observe that there is a high positive correlation between the `customer value` and the `frequency of SMS` as well as `seconds of use`, and `frequency of use` variables. \n\nthe high correlation between some of the numerical variables may lead to `multicollinearity` issues which need to be adressed during the modeling stage.\n\nMoreover, we have found that `the status` and `complaints` variables have a strong correlation with the target variable `churn`. This may indicates that customers who have complaints or are non-active are more likely to churn.","metadata":{"id":"yjYWAIqC6sKu"}},{"cell_type":"markdown","source":"# ðŸ”¢ Clustering ","metadata":{"id":"ZXVucCiu85Gc"}},{"cell_type":"code","source":"!pip install pycaret","metadata":{"execution":{"iopub.status.busy":"2023-03-30T17:00:07.88332Z","iopub.execute_input":"2023-03-30T17:00:07.883712Z","iopub.status.idle":"2023-03-30T17:00:52.414492Z","shell.execute_reply.started":"2023-03-30T17:00:07.883679Z","shell.execute_reply":"2023-03-30T17:00:52.41332Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from pycaret.clustering import *\n# Set up Pycaret environment for clustering \ncluster_setup = setup(df, \n                      normalize = True, # Normalize the data\n                      pca = True, # Perform PCA dimensionality reduction\n                      pca_method = 'linear', # Use linear PCA\n                      pca_components = 2, # Reduce to 2 principal components\n                      ignore_features = ['Age Group','Status','Tariff Plan','Age','Churn','Complaints','Call Failure'], # Ignore any features you don't want to use\n                      session_id = 123) # Set a random seed for reproducibility","metadata":{"id":"riGOSTRbJLya","outputId":"cd3d785d-8e44-4436-97e6-94251f92534c","execution":{"iopub.status.busy":"2023-03-30T17:00:53.95881Z","iopub.execute_input":"2023-03-30T17:00:53.959286Z","iopub.status.idle":"2023-03-30T17:01:02.030893Z","shell.execute_reply.started":"2023-03-30T17:00:53.959232Z","shell.execute_reply":"2023-03-30T17:01:02.029532Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"models()","metadata":{"id":"_1XNA40AJhfK","outputId":"56653461-394c-47f7-ac37-db4b5088e9b3","execution":{"iopub.status.busy":"2023-03-30T17:01:49.693333Z","iopub.execute_input":"2023-03-30T17:01:49.693868Z","iopub.status.idle":"2023-03-30T17:01:49.708976Z","shell.execute_reply.started":"2023-03-30T17:01:49.693817Z","shell.execute_reply":"2023-03-30T17:01:49.707779Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Define list of clustering models and their hyperparameters\nmodels = {\n    'kmeans': {'num_clusters': [2,3, 4, 5, 6, 7]},\n    'sc': {'num_clusters': [2,3, 4, 5, 6, 7]},\n    'hclust': {'linkage': ['ward', 'complete', 'average']},\n    'dbscan': {'eps': [0.1, 0.3,0.5,0.7,0.9]},\n}\n\n# Train and evaluate clustering models\nfor model_name, hyperparams in models.items():\n    for param_name, param_values in hyperparams.items():\n        for param_value in param_values:\n            hyperparam_dict = {param_name: param_value}\n            model = create_model(model_name, **hyperparam_dict)\n            print(model_name, hyperparam_dict)\n","metadata":{"id":"TJg_aP91MgL4","outputId":"a5d637c3-0f23-42a8-a810-8b2ee75c78d4","execution":{"iopub.status.busy":"2023-03-30T17:01:52.064136Z","iopub.execute_input":"2023-03-30T17:01:52.064534Z","iopub.status.idle":"2023-03-30T17:02:23.123711Z","shell.execute_reply.started":"2023-03-30T17:01:52.064493Z","shell.execute_reply":"2023-03-30T17:02:23.122105Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"\n\n> These three indices are commonly used to evaluate the quality of clustering algorithms. \n\n\n*  `Silhouette score` : A measure of how similar an object is to its own cluster compared to other clusters. It ranges from -1 to 1, with higher values indicating better cluster quality.\n*   `Calinski-Harabasz`: An index that measures the ratio of between-cluster variance to within-cluster variance. Higher values indicate better-defined clusters.\n* `Davies-Bouldin`: A measure of the average similarity between each cluster and its most similar cluster. Lower values indicate better clustering.\n\n\n\n","metadata":{"id":"RSF-3SdTU_1-"}},{"cell_type":"markdown","source":"* the `k-means` clustering algorithm with `3 clusters` seems to perform the best among the tested clustering algorithms. It has the `highest silhouette score`, which indicates that the clusters are well separated and data points are assigned to the correct cluster. It also has a `high Calinski-Harabasz score`, which is an indicator of the compactness and separation between clusters. `The Davies-Bouldin` score is also quite good, indicating that the clusters are well separated and distinct.\n\n\n* `hierarchical clustering` with ward linkage also shows promising results, exhibiting a high silhouette score and low Davies-Bouldin score. Since hierarchical clustering does not require us to specify the number of clusters beforehand, we can plot the `dendrogram` to visualize the hierarchical structure of the clustering and pick the `optimal number of clusters`","metadata":{"id":"o8oYIDiQUKKr"}},{"cell_type":"code","source":"from sklearn.preprocessing import StandardScaler\nimport matplotlib.pyplot as plt\nfrom scipy.cluster.hierarchy import dendrogram, linkage\n\nscaler = StandardScaler()\nX = df.drop(['Age Group','Status','Tariff Plan','Age','Churn','Complaints','Call Failure'],axis=1)\nX_scaled = scaler.fit_transform(X)\n# Perform hierarchical clustering\nZ = linkage(X_scaled, method='ward')\n\n# Plot the dendrogram\nplt.figure(figsize=(12, 6))\ndendrogram(Z)\nplt.title('Dendrogram')\nplt.xlabel('Employee Index')\nplt.ylabel('Distance')\nplt.show()","metadata":{"id":"1yN3ECfHtvtD","outputId":"38e16c35-d832-4363-990b-6a4b4d1aa1b4","execution":{"iopub.status.busy":"2023-03-30T17:02:57.569438Z","iopub.execute_input":"2023-03-30T17:02:57.569837Z","iopub.status.idle":"2023-03-30T17:04:00.357701Z","shell.execute_reply.started":"2023-03-30T17:02:57.569801Z","shell.execute_reply":"2023-03-30T17:04:00.356189Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Creating the k-means model with the optimal number of clusters \nkmeans = create_model('kmeans', num_clusters = 3)","metadata":{"id":"fb1MHwYCwLm8","outputId":"6bda5f0f-9efe-453c-8949-ea8c081ef97f","execution":{"iopub.status.busy":"2023-03-30T17:04:14.00856Z","iopub.execute_input":"2023-03-30T17:04:14.009011Z","iopub.status.idle":"2023-03-30T17:04:14.78161Z","shell.execute_reply.started":"2023-03-30T17:04:14.008958Z","shell.execute_reply":"2023-03-30T17:04:14.780226Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Plotting clusters using PCA for dimensionality reduction\nplot_model(kmeans, plot = 'cluster', scale = 2)","metadata":{"id":"IG6wl1JvwYOr","outputId":"693f60f1-2c2e-4a36-8bcd-2d7aea3eaa88","execution":{"iopub.status.busy":"2023-03-30T17:04:19.403738Z","iopub.execute_input":"2023-03-30T17:04:19.404167Z","iopub.status.idle":"2023-03-30T17:04:21.532338Z","shell.execute_reply.started":"2023-03-30T17:04:19.40413Z","shell.execute_reply":"2023-03-30T17:04:21.530878Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Plotting the distribution of each cluster\nplot_model(kmeans, plot = 'distribution', scale = 2)","metadata":{"id":"AjnRH5_Pwp6q","outputId":"28127426-bad3-4078-8195-e3b583df0a27","execution":{"iopub.status.busy":"2023-03-30T17:04:26.724647Z","iopub.execute_input":"2023-03-30T17:04:26.725062Z","iopub.status.idle":"2023-03-30T17:04:27.167579Z","shell.execute_reply.started":"2023-03-30T17:04:26.725025Z","shell.execute_reply":"2023-03-30T17:04:27.166238Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Assigning each observation to a cluster\nkmeans_cluster = assign_model(kmeans)\nkmeans_cluster.head()","metadata":{"id":"JRKXE_UZxpC3","outputId":"16267ecf-777e-4e4f-e23a-55cfee311634","execution":{"iopub.status.busy":"2023-03-30T17:04:31.024034Z","iopub.execute_input":"2023-03-30T17:04:31.02447Z","iopub.status.idle":"2023-03-30T17:04:31.043591Z","shell.execute_reply.started":"2023-03-30T17:04:31.024427Z","shell.execute_reply":"2023-03-30T17:04:31.042116Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Calculating means for each variable \nmeans_by_cluster = kmeans_cluster.groupby('Cluster').mean()\nmeans_by_cluster\n","metadata":{"id":"B8ZLa-YBxtl7","outputId":"17b605d4-bf3a-4010-fa45-15caadb034a4","execution":{"iopub.status.busy":"2023-03-30T17:04:33.443922Z","iopub.execute_input":"2023-03-30T17:04:33.444376Z","iopub.status.idle":"2023-03-30T17:04:33.469162Z","shell.execute_reply.started":"2023-03-30T17:04:33.444334Z","shell.execute_reply":"2023-03-30T17:04:33.467889Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Creating the hclust model with optimal params \nhclust = create_model('hclust', linkage = \"ward\", num_clusters = 3)","metadata":{"id":"bsCawyZewuo5","outputId":"9a7cb13c-43ce-49ee-e55e-78d8539e5202","execution":{"iopub.status.busy":"2023-03-30T17:04:35.484466Z","iopub.execute_input":"2023-03-30T17:04:35.484871Z","iopub.status.idle":"2023-03-30T17:04:36.393927Z","shell.execute_reply.started":"2023-03-30T17:04:35.484836Z","shell.execute_reply":"2023-03-30T17:04:36.392587Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Plotting clusters using PCA for dimensionality reduction\nplot_model(hclust, plot = 'cluster', scale = 2)","metadata":{"id":"e3PpV4f5w_a5","outputId":"acf50227-2dea-4a9b-bf0b-1c1f661dc22e","execution":{"iopub.status.busy":"2023-03-30T17:04:38.284062Z","iopub.execute_input":"2023-03-30T17:04:38.284459Z","iopub.status.idle":"2023-03-30T17:04:38.617184Z","shell.execute_reply.started":"2023-03-30T17:04:38.284424Z","shell.execute_reply":"2023-03-30T17:04:38.615945Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Plotting the distribution of each cluster\nplot_model(hclust, plot = 'distribution', scale = 2)","metadata":{"id":"irHRefUCxDp6","outputId":"64131b85-be67-418c-a911-25d4370683ad","execution":{"iopub.status.busy":"2023-03-30T17:04:42.636513Z","iopub.execute_input":"2023-03-30T17:04:42.636961Z","iopub.status.idle":"2023-03-30T17:04:43.007026Z","shell.execute_reply.started":"2023-03-30T17:04:42.63692Z","shell.execute_reply":"2023-03-30T17:04:43.005649Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Assigning each observation to a cluster\nhc_cluster = assign_model(hclust)\n# Calculating means for each variable \nmeans_by_cluster_hc = hc_cluster.groupby('Cluster').mean()\nmeans_by_cluster_hc\n","metadata":{"id":"lnQ-fR8WxMNR","outputId":"a8275b87-d6c6-4fe4-d379-0ed66db9aac1","execution":{"iopub.status.busy":"2023-03-30T17:04:46.484268Z","iopub.execute_input":"2023-03-30T17:04:46.48468Z","iopub.status.idle":"2023-03-30T17:04:46.507381Z","shell.execute_reply.started":"2023-03-30T17:04:46.484645Z","shell.execute_reply":"2023-03-30T17:04:46.506049Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"\n\n> Intepretation:\n\nHere's a possible interpretation for hierarchical clustering results :\n\n* `Cluster 0`: Customers with short subscriptions, low usage, and low customer value. They may use their subscription occasionally and prioritize affordability.\n\n* `Cluster 1`: Customers with slightly longer subscriptions, high usage, and high customer value. They rely heavily on their subscription for communication, value features, and are likely loyal.\n\n* `Cluster 2`: Customers with the longest subscriptions, highest usage, and very high customer value. They are power users who rely heavily on their subscription for both personal and professional communication, willing to pay for premium features, and may have high loyalty to their provider.\n\nUpon examining the k-means clustering results, it is clear that the clusters share common characteristics, though the order in which they are identified may vary.","metadata":{"id":"ATZM_WbDBpmH"}},{"cell_type":"markdown","source":"# ðŸ“ˆ Classification :\n","metadata":{"id":"dki_K0NmC_w-"}},{"cell_type":"markdown","source":"When building a machine learning model, it's essential to test multiple algorithms and identify the ones that perform well on the dataset. However, with so many algorithms available, it can be challenging to determine which ones to use and how to customize them for optimal performance.\n\nTo address this challenge, I will take a systematic and data-driven approach to build my machine learning model. First, I will test various algorithms without any customization and evaluate their performance. This allows me to identify the models that perform well on the dataset and shortlist them for further tuning.","metadata":{"id":"5-5qQguVWdQO"}},{"cell_type":"markdown","source":"## Without Resampling","metadata":{"id":"a1-ft6gRR4nv"}},{"cell_type":"code","source":"# Setting up the preprocessing pipeline\nfrom pycaret.classification import *\nclassification_setup = setup(data=df,\n      target='Churn',\n      normalize=True, # normalize numeric columns\n      categorical_features=['Complaints', 'Charge Amount', 'Age Group', 'Tariff Plan', 'Status'], # specify categorical columns\n      remove_multicollinearity = True, # remove multicolinearity for a default threshold of 0.9\n      low_variance_threshold = 0.1,\n      fix_imbalance = False\n)","metadata":{"id":"-RTQLxQIS-nx","outputId":"0f0c8223-3931-4430-c6ff-3b7088382a89","execution":{"iopub.status.busy":"2023-03-30T17:07:29.850366Z","iopub.execute_input":"2023-03-30T17:07:29.850934Z","iopub.status.idle":"2023-03-30T17:07:31.811069Z","shell.execute_reply.started":"2023-03-30T17:07:29.850881Z","shell.execute_reply":"2023-03-30T17:07:31.809943Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"* `Accuracy` can be misleading when the data is `imbalanced`, as it doesn't consider the relative importance of different types of classification errors. In the case of a churn problem, where churn represents only `13%` of the data and non-churn represents `87%`, accuracy can give a false impression of high performance if the model mostly predicts non-churn.\n\n* One solution to address the issue of imbalanced data is to use alternative evaluation metrics that are more suitable for imbalanced datasets, such as `precision`, `recall`, and `F1-score`. These metrics take into account the different types of classification errors and provide a more comprehensive picture of the model's performance.\n\n","metadata":{"id":"4GO_ruqxWonX"}},{"cell_type":"code","source":"# Fitting all models and comparing results using K-fold Cross Validation with 10 folds \nbest_model = compare_models(sort='F1')","metadata":{"id":"PtcK5IKSTyyK","outputId":"ef3d9932-d43d-4f7d-fb23-68e1ccceeb48","execution":{"iopub.status.busy":"2023-03-30T17:07:37.201174Z","iopub.execute_input":"2023-03-30T17:07:37.201569Z","iopub.status.idle":"2023-03-30T17:09:07.980919Z","shell.execute_reply.started":"2023-03-30T17:07:37.201533Z","shell.execute_reply":"2023-03-30T17:09:07.980069Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"* For this particular problem, it is important to balance both precision and recall in order to effectively identify customers who are likely to churn. False positives and false negatives have equal importance, meaning that both types of errors need to be minimized in order to develop an accurate churn prediction model. In this context, F1-score can be a useful metric to optimize for, as it takes into account both precision and recall, providing a balanced measure of the model's performance on both classes.\n\n* Based on the results of the evaluation metrics, it appears that `CatBoost Classifier` has the best F1 score of `0.8190` as well as a high accuracy of 0.9460.`Light Gradient Boosting Machine` and `Extreme Gradient Boosting` also perform well, with F1 scores of `0.8031` and `0.7973`, respectively, and high accuracies of `0.9406` and `0.9388`. These models may also be worth further consideration as they have strong overall performance.","metadata":{"id":"tlA7tQVvZpV3"}},{"cell_type":"markdown","source":"## Resampling Techniques ( Oversampling / Undersampling / SMOTE )","metadata":{"id":"DY9CWiq1SaIG"}},{"cell_type":"markdown","source":"* Resampling techniques are a set of methods used to `balance` the distribution of classes in an imbalanced dataset. \n* Resampling techniques can be used to adjust the class distribution. These techniques fall into two broad categories: `oversampling` and `undersampling`. Oversampling techniques `increase` the representation of the `minority class` by duplicating or creating synthetic examples, while undersampling techniques `reduce` the number of `majority class` samples.","metadata":{"id":"L2070ks8YtGh"}},{"cell_type":"markdown","source":"### SMOTE \n","metadata":{"id":"RFZvrQ4WOGyo"}},{"cell_type":"code","source":"SMOTE_setup = setup(data=df,\n      target='Churn',\n      normalize=True, # normalize numeric columns\n      categorical_features=['Complaints', 'Charge Amount', 'Age Group', 'Tariff Plan', 'Status'], # specify categorical columns\n      remove_multicollinearity = True, # remove multicolinearity for a default threshold of 0.9\n      low_variance_threshold = 0.1,\n      fix_imbalance = True,\n      fix_imbalance_method = 'SMOTE' \n)","metadata":{"id":"p6v2Q19NEM8T","outputId":"0bdaa53c-fa28-4f0d-ec39-0a266c5af339"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"best_model_1 = compare_models()","metadata":{"id":"gTd2n15yHmVO","outputId":"15e34bec-0904-4d53-9efa-9bd05c5242a1"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"* Based on the new results, even after applying the `SMOTE` resampling technique and using accuracy as the evaluation metric, `CatBoost` still outperformed `LightGBM` and `XGBoost`.\n","metadata":{"id":"NX6PPgiFcv6J"}},{"cell_type":"markdown","source":"### Under Sampling","metadata":{"id":"XRb95ChBPTYn"}},{"cell_type":"code","source":"RUS_setup = setup(data=df,\n      target='Churn',\n      normalize=True, # normalize numeric columns\n      categorical_features=['Complaints', 'Charge Amount', 'Age Group', 'Tariff Plan', 'Status'], # specify categorical columns\n      remove_multicollinearity = True, # remove multicolinearity for a default threshold of 0.9\n      low_variance_threshold = 0.1,\n      fix_imbalance = True,\n      fix_imbalance_method = 'RandomUnderSampler' \n)","metadata":{"id":"nNu2ht5hLm-z","outputId":"641a2077-1744-49bb-f0d3-9fd9ec4aacd1"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"best_model_2 = compare_models()","metadata":{"id":"5oJKLsn1PeAB","outputId":"66bde6e6-df26-47e8-cf1a-59059e8929a0"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Based on the results obtained after applying `undersampling` on the dataset using `RandomUnderSampler`, it can be concluded that this technique did not improve the performance of any of the models. In fact, the accuracy and other metrics were lower than those obtained without any resampling","metadata":{"id":"B09CY_z7d5_z"}},{"cell_type":"markdown","source":"### Over Sampling","metadata":{"id":"aOY06FvZQHyL"}},{"cell_type":"code","source":"ROS_setup = setup(data=df,\n      target='Churn',\n      normalize=True, # normalize numeric columns\n      categorical_features=['Complaints', 'Charge Amount', 'Age Group', 'Tariff Plan', 'Status'], # specify categorical columns\n      remove_multicollinearity = True, # remove multicolinearity for a default threshold of 0.9\n      low_variance_threshold = 0.1,\n      fix_imbalance = True,\n      fix_imbalance_method = 'RandomOverSampler' \n)","metadata":{"id":"w5t89u2wQP-a","outputId":"11e905bc-4657-4c5c-b351-ebb118c22460"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"best_model_3 = compare_models()","metadata":{"id":"cHvGn-WVQXIa","outputId":"877f907f-acf7-4305-91a2-3f4c942e61d6"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"After comparing the performance of the three models using the random oversampling technique, we found that `XGBoost` and `LightGBM` now have an accuracy of `0.9492`, which outperforms `CatBoost`. This suggests that the random oversampler was a more effective technique for improving the performance of these models on our imbalanced data. ","metadata":{"id":"M4gpYamOe97j"}},{"cell_type":"markdown","source":"##  Optimization by hyperparameter tuning","metadata":{"id":"ipK8THhQg_r-"}},{"cell_type":"markdown","source":"### Catboost with SMOTE setup","metadata":{"id":"7K5w4HW1h19C"}},{"cell_type":"markdown","source":"Now that we have established several baseline models that are performing well, we can further improve their performance by tuning their hyperparameters. This can help us achieve even better results and potentially identify the best model for our particular problem.","metadata":{"id":"kZhZrhnlhNMn"}},{"cell_type":"code","source":"SMOTE_setup = setup(data=df,\n      target='Churn',\n      normalize=True, # normalize numeric columns\n      categorical_features=['Complaints', 'Charge Amount', 'Age Group', 'Tariff Plan', 'Status'], # specify categorical columns\n      remove_multicollinearity = True, # remove multicolinearity for a default threshold of 0.9\n      low_variance_threshold = 0.1,\n      fix_imbalance = True,\n      fix_imbalance_method = 'SMOTE' \n)","metadata":{"id":"qc5CVunLhp0r","outputId":"1bfae847-e6df-4ed2-c646-9c945146d3ca"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"catboost_smote = create_model('catboost')","metadata":{"id":"V3ylAc3Fh6Ei","outputId":"cc537152-ae66-40b4-c5c9-f8e94913c77d"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"tuned_catboost_smote = tune_model(catboost_smote)","metadata":{"id":"Yk9EN6pFiIi5","outputId":"55d14637-a3a8-4946-b444-db47b6031d6a"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"After performing hyperparameter tuning, the performance of the model did not improve and even decreased compared to the original model. As a result, it was decided to use the original model instead of the tuned model.","metadata":{"id":"96yJSQpel8Hq"}},{"cell_type":"markdown","source":"### XGboost with RandomOverSampling setup","metadata":{"id":"oSowZHyilp67"}},{"cell_type":"code","source":"ROS_setup = setup(data=df,\n      target='Churn',\n      normalize=True, # normalize numeric columns\n      categorical_features=['Complaints', 'Charge Amount', 'Age Group', 'Tariff Plan', 'Status'], # specify categorical columns\n      remove_multicollinearity = True, # remove multicolinearity for a default threshold of 0.9\n      low_variance_threshold = 0.1,\n      fix_imbalance = True,\n      fix_imbalance_method = 'RandomOverSampler' \n)","metadata":{"id":"xMGS055zlwKY","outputId":"4066b941-24f0-497d-bb33-fc26a2f41426"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"xgb_ros = create_model('xgboost')","metadata":{"id":"K6Cdi2cqmMK2","outputId":"fc9f6067-abc7-4e63-f798-d83b902e55ad"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"tuned_xgb_ros = tune_model(xgb_ros)","metadata":{"id":"O2NlenrKmie-","outputId":"b36a883f-5df1-442f-be70-2f17f6397cde"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"After tuning its hyperparameters, the performance of the XGBoost model did not show any improvement as well.\n\n\n\n","metadata":{"id":"gG94Dv6ynSa8"}},{"cell_type":"markdown","source":"### Catboost without resampling","metadata":{"id":"WfNGM6ShnuLz"}},{"cell_type":"code","source":"classification_setup = setup(data=df,\n      target='Churn',\n      normalize=True, # normalize numeric columns\n      categorical_features=['Complaints', 'Charge Amount', 'Age Group', 'Tariff Plan', 'Status'], # specify categorical columns\n      remove_multicollinearity = True, # remove multicolinearity for a default threshold of 0.9\n      low_variance_threshold = 0.1,\n      fix_imbalance = False\n)","metadata":{"id":"dnij_nmUn8j5","outputId":"a995fc86-fca9-4699-a613-39bc6868f36b"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"catboost = create_model('catboost')","metadata":{"id":"Sk7eSsHDoDia","outputId":"f6c5e8a3-1aa6-4016-b0b9-c7f672444e28"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"catboost_tuned = tune_model(catboost)","metadata":{"id":"CSC7Ou5woJ4K","outputId":"54189747-3cf4-4518-9fda-ecbe5cde7061"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"The hyperparameter tuning did not lead to an improvement in the performance of the CatBoost model","metadata":{"id":"nkJrN7HJo2kX"}},{"cell_type":"markdown","source":"\n\n> Conclusion :\n\nBased on the results obtained after tuning the hyperparameters of both the CatBoost and XGBoost models, it can be concluded that the default parameter settings of these models provide the best performance for the given problem. The attempts to optimize the models by changing the hyperparameters did not result in any significant improvement in their performance.","metadata":{"id":"2mn7ae4ZpDpW"}},{"cell_type":"markdown","source":"## Optimization by aggregating models ","metadata":{"id":"ARS5kd4Ip5sM"}},{"cell_type":"markdown","source":"### Probabilites","metadata":{"id":"99nMEFoLsA4z"}},{"cell_type":"markdown","source":"When the `method` argument is set to `'soft'`, `blend_models` predicts the class label based on the argmax of the sums of the predicted probabilities.","metadata":{"id":"LgqVQtnmso9d"}},{"cell_type":"code","source":"blender_s = blend_models([catboost_smote, xgb_ros ,catboost], fold = 10 ,method='soft')","metadata":{"id":"vJBM8aAOqA3c","outputId":"bca6e458-8662-4fbf-b48b-0c7762b01801"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Labels","metadata":{"id":"XsVaRa--sNoN"}},{"cell_type":"markdown","source":"When the `method` argument is set to `'hard'`, `blend_models`  uses the predictions (hard labels) from input models instead of probabilities.","metadata":{"id":"T5NcFDgEtXFa"}},{"cell_type":"code","source":"blender_h = blend_models([catboost_smote, xgb_ros ,catboost], fold = 10 ,method='hard')","metadata":{"id":"xqWBj06wt335","outputId":"4da584ff-293a-4b30-b19c-106231026819"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"\n\n> Conclusion :\n\nBased on the results obtained, `the `top-performing models` together has shown to slightly improve the overall performance of the model. The blended model outperforms each of the individual models, indicating that it is an effective approach for improving model performance.","metadata":{"id":"Z6SgPq4Qu6dr"}},{"cell_type":"markdown","source":"## Analyzing the top performing models ","metadata":{"id":"4-wIHcDzwmbp"}},{"cell_type":"markdown","source":"### Evaluation on the holdout set ( test set )","metadata":{"id":"eGTuorH5wqXp"}},{"cell_type":"code","source":"# Plotting the confusion matrix\nplot_model(catboost_smote, plot = 'confusion_matrix', plot_kwargs = {'percent' : True})","metadata":{"id":"C3wFc65hy_Rj","outputId":"87964cac-4023-42ab-8078-95268a17c80c"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"plot_model(catboost, plot = 'confusion_matrix', plot_kwargs = {'percent' : True})","metadata":{"id":"wBUHho2Bz0iB","outputId":"bc00161e-2c20-45d2-dc74-5854aa24c809"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"plot_model(xgb_ros, plot = 'confusion_matrix', plot_kwargs = {'percent' : True})","metadata":{"id":"_rnFDLeH0ulv","outputId":"cf2b4347-b9bf-401f-80c2-fb987f0ff585"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"\n\n> Conclusion :\n\nTo prioritize true positives, XGBoost may be the better choice. On the other hand, if the goal is to prioritize true negatives, CatBoost without resampling could be the preferred model. However, using CatBoost with SMOTE resampling may result in a high number of false negatives, making it a less suitable option.\n","metadata":{"id":"cXHFx2gN1yma"}},{"cell_type":"markdown","source":"## Models interpretability","metadata":{"id":"42A_z-hC28om"}},{"cell_type":"code","source":"# Plotting model intepretation\ninterpret_model(xgb_ros)","metadata":{"id":"RxZsQu8y3IMO","outputId":"d7bcf0b8-0076-4240-c220-409f4bee3d7c"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"interpret_model(catboost)","metadata":{"id":"inyWv-mh318c","outputId":"5ce0a46a-c7d2-4217-bce7-d4f324c3d9ca"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Plotting most import features for prediction\nplot_model(xgb_ros, plot='feature')","metadata":{"id":"u-i0eCHe4hdS","outputId":"6dfe91f0-9d6f-4654-9495-cd2d1c186cec"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"plot_model(catboost, plot='feature')","metadata":{"id":"djPgUlrE5Jnm","outputId":"b4fe56fe-5438-42e9-dddf-ea56272995c0"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"\n\n> Conclusion :\n\n* `XGboost` : the `\"Status\"` is highly important in predicting the target variable. The feature's contribution to the model's performance is relatively high compared to other features.\n\n* `Catboost` : Based on the feature importance analysis using CatBoost, it can be concluded that the top features contributing to customer churn are seconds of use, subscription length, distinct called number, and call failure. However, it should be noted that there is not a significant difference in the feature importance among these top features. This suggests that these features are equally important in predicting customer churn and should be given equal consideration when developing a churn prediction model.\n","metadata":{"id":"yEWcH9tL5hbx"}}]}